library(readr) # CSV file I/O, e.g. the read_csv function
library(h2o) #deep learning
library(caret) #preprocessing
library(dplyr) # data manipulation
library(lubridate) # dates
library(rpart) # rpart for imputation

#Load Data


# Read the data
train <- read.csv('../input/train.csv', stringsAsFactors = F)
test <- read.csv('../input/test.csv', stringsAsFactors = F)


##Preprocessing taken from Megan Risdal's "Quick and Dirty RandomForest"
# Rename the ID column so train & test match
names(train)[1] <- 'ID'

# And change ID in test to character
test$ID <- as.character(test$ID)

# Combine test & training data
full <- bind_rows(train, test)

# Get the time value:
full$TimeValue <- sapply(full$AgeuponOutcome,  
                      function(x) strsplit(x, split = ' ')[[1]][1])

# Now get the unit of time:
full$UnitofTime <- sapply(full$AgeuponOutcome,  
                      function(x) strsplit(x, split = ' ')[[1]][2])

# Fortunately any "s" marks the plural, so we can just pull them all out
full$UnitofTime <- gsub('s', '', full$UnitofTime)

full$TimeValue  <- as.numeric(full$TimeValue)
full$UnitofTime <- as.factor(full$UnitofTime)

# Make a multiplier vector
multiplier <- ifelse(full$UnitofTime == 'day', 1,
              ifelse(full$UnitofTime == 'week', 7,
              ifelse(full$UnitofTime == 'month', 30, # Close enough
              ifelse(full$UnitofTime == 'year', 365, NA))))

# Apply our multiplier
full$AgeinDays <- full$TimeValue * multiplier

# Replace blank names with "Nameless"
full$Name <- ifelse(nchar(full$Name)==0, 'Nameless', full$Name)

# Make a name v. no name variable
full$HasName[full$Name == 'Nameless'] <- 0
full$HasName[full$Name != 'Nameless'] <- 1

# Replace blank sex with most common
full$SexuponOutcome <- ifelse(nchar(full$SexuponOutcome)==0, 
                              'Spayed Female', full$SexuponOutcome)

# Extract time variables from date (uses the "lubridate" package)
full$Hour    <- hour(full$DateTime)
full$Weekday <- wday(full$DateTime)
full$Month   <- month(full$DateTime)
full$Year    <- year(full$DateTime)

# Time of day may also be useful
full$TimeofDay <- ifelse(full$Hour > 5 & full$Hour < 11, 'morning',
                  ifelse(full$Hour > 10 & full$Hour < 16, 'midday',
                  ifelse(full$Hour > 15 & full$Hour < 20, 'lateday', 'night')))

# Put factor levels into the order we want
full$TimeofDay <- factor(full$TimeofDay, 
                    levels = c('morning', 'midday',
                               'lateday', 'night'))

# Take a look as some of the levels
levels(factor(full$Breed))[1:10]

# Use "grepl" to look for "Mix"
full$IsMix <- ifelse(grepl('Mix', full$Breed), 1, 0)

# Split on "/" and remove " Mix" to simplify Breed
full$SimpleBreed <- sapply(full$Breed, 
                      function(x) gsub(' Mix', '', 
                        strsplit(x, split = '/')[[1]][1]))
                        
                        
                        
# Use strsplit to grab the first color
full$SimpleColor <- sapply(full$Color, 
                      function(x) strsplit(x, split = '/| ')[[1]][1])
levels(factor(full$SimpleColor))
# Use "grepl" to look for "Intact"
full$Intact <- ifelse(grepl('Intact', full$SexuponOutcome), 1,
               ifelse(grepl('Unknown', full$SexuponOutcome), 'Unknown', 0))

# Use "grepl" to look for sex
full$Sex <- ifelse(grepl('Male', full$SexuponOutcome), 'Male',
            ifelse(grepl('Unknown', full$Sex), 'Unknown', 'Female'))
            
# Use rpart to predict the missing age values
age_fit <- rpart(AgeinDays ~ AnimalType + Sex + Intact + SimpleBreed + HasName, 
              data = full[!is.na(full$AgeinDays), ], 
              method = 'anova')

# Impute predicted age values where missing using "predict"
full$AgeinDays[is.na(full$AgeinDays)] <- predict(age_fit, full[is.na(full$AgeinDays), ])

# All gone? Yes.
sum(is.na(full$AgeinDays))

# Use the age variable to make a puppy/kitten variable
full$Lifestage[full$AgeinDays < 365] <- 'baby'
full$Lifestage[full$AgeinDays >= 365] <- 'adult'

full$Lifestage <- factor(full$Lifestage)

factorVars <- c('Name','OutcomeType','OutcomeSubtype','AnimalType',
                'SexuponOutcome','AgeuponOutcome','SimpleBreed','SimpleColor',
                'HasName','IsMix','Intact','Sex','TimeofDay','Lifestage')

full[factorVars] <- lapply(full[factorVars], function(x) as.factor(x))





#Start up h20 cluster for deep learning


h2o.init(nthreads = -1,min_mem_size="1G")
dat_h2o = as.h2o(full) #Convert to h2o dataframe




dat_h2o=dat_h2o[,-5] #remove to prevent leakage
dat_h2o=dat_h2o[,-3]#data is elsewhere
dat_h2o=dat_h2o[,-2]#no reason to suppose this is a predictor
dat_h2o=dat_h2o[,-1]#no reason to suppose this is a predictor

# Split up train and test data
train_master <- dat_h2o[1:26729, ]
testsubmit  <- dat_h2o[26730:nrow(dat_h2o), ]

splits = h2o.splitFrame(train_master, c(0.6,0.2), seed=1234) #split into train and test
train  = h2o.assign(splits[[1]], "train.hex") # 60%
valid  = h2o.assign(splits[[2]], "valid.hex") # 20%
test   = h2o.assign(splits[[3]], "test.hex")  # 20%

response <- "OutcomeType"
predictors <- setdiff(names(train), response)
predictors



#Logloss ~1.5, validation and train sets close

#Try first model
#m1 <- h2o.deeplearning(
#  model_id="dl_model_first", 
#  training_frame=train, 
#  validation_frame=valid,   ## validation dataset: used for scoring and early stopping
#  x=predictors,
#  y=response,
#  activation="Rectifier",  ## default
#  hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each
#  epochs=1,
#  variable_importances=T    ## not enabled by default
#)


#examine model
#plot(m1)
#summary(m1)

#Logloss ~1, validation and train sets close



#Look at principal components
#h2o.prcomp(training_frame = train, k = 20, transform = "STANDARDIZE")
#First 10 account for 90% of the 87% of explained variation.


#Autoencoder
#ae_model = h2o.deeplearning(model_id="ae_model",x = 1:ncol(train), standardize=TRUE, training_frame = train, autoencoder = TRUE,
#hidden = c(20, 10,20), epochs = 1,activation="Tanh",variable_importances=T)

#Note that highly importance variables in the autoencoded model signify clusters.
#Cannot get h2o autoencoder to pretrain deep net. Ticket opened with H20.
#h2o.varimp(ae_model)

#Determine relevent features, pick top 10 for deep learning 
#RF=h2o.randomForest(model_id="RF", 
# training_frame=train, 
#  validation_frame=valid, 
#  x=predictors,
#  y=response,

#    mtries = -1,	
#	sample_rate = 0.632,
# 	build_tree_one_node = FALSE,
#    ntrees=1000,
#	max_depth = 2,
# 	min_rows = 1,
#	nbins = 20,
#	balance_classes = TRUE,
#	score_each_iteration = FALSE,
#	seed=147)
#summary(RF)



#write.table(h2o.varimp(RF), file="Variable_Importances")
#1  AgeuponOutcome       696141.000000          1.000000   0.205627
#2          Intact       488126.750000          0.701189   0.144183
#3  SexuponOutcome       438713.937500          0.630208   0.129588
#4       AgeinDays       413046.750000          0.593338   0.122006
#5     SimpleBreed       330815.031250          0.475213   0.097716
#6         HasName       268610.625000          0.385857   0.079342
#7      UnitofTime       188643.703125          0.270985   0.055722
#8       Lifestage       143638.359375          0.206335   0.042428
#9      AnimalType       135596.078125          0.194782   0.040052
#10           Hour       124551.976562          0.178918   0.036790
#11      TimeofDay       117922.703125          0.169395   0.034832
#12      TimeValue        18708.027344          0.026874   0.005526
#13    SimpleColor        11895.077148          0.017087   0.003514
#14            Sex         2925.156006          0.004202   0.000864
#15           Year         2803.634033          0.004027   0.000828
#16          Month         1493.404541          0.002145   0.000441
#17        Weekday         1443.735107          0.002074   0.000426
#18          IsMix          387.228821          0.000556   0.000114



# [1] "ID"             "Name"           "DateTime"       "OutcomeSubtype"
# [5] "AnimalType"     "SexuponOutcome" "AgeuponOutcome" "Breed"         
# [9] "Color"          "TimeValue"      "UnitofTime"     "AgeinDays"     
#[13] "HasName"        "Hour"           "Weekday"        "Month"         
#[17] "Year"           "TimeofDay"      "IsMix"          "SimpleBreed"   
#[21] "SimpleColor"    "Intact"         "Sex"            "Lifestage"


#Take top 10- note this may be slightly different since these results are not reproducable
dat_h2o = as.h2o(full) #Convert to h2o dataframe
dat_h2o=dat_h2o[,-24] #remove unimportant variables
dat_h2o=dat_h2o[,-22]
dat_h2o=dat_h2o[,-20]
dat_h2o=dat_h2o[,-19]
dat_h2o=dat_h2o[,-18]
dat_h2o=dat_h2o[,-17]
dat_h2o=dat_h2o[,-16]
dat_h2o=dat_h2o[,-11]
dat_h2o=dat_h2o[,-10]
dat_h2o=dat_h2o[,-9]
dat_h2o=dat_h2o[,-5] #remove to prevent leakage
dat_h2o=dat_h2o[,-3]#data is elsewhere
dat_h2o=dat_h2o[,-2]#no reason to suppose this is a predictor
dat_h2o=dat_h2o[,-1]#no reason to suppose this is a predictor


train_master <- dat_h2o[1:26729, ]
testsubmit  <- dat_h2o[26730:nrow(dat_h2o), ]

splits = h2o.splitFrame(train_master, c(0.6,0.2), seed=1234) #split into train and test
train  = h2o.assign(splits[[1]], "train.hex") # 60%
valid  = h2o.assign(splits[[2]], "valid.hex") # 20%
test   = h2o.assign(splits[[3]], "test.hex")  # 20%
attach(train)
response <- "OutcomeType"
predictors <- setdiff(names(train), response)
predictors


#Supervised blending
kmean=h2o.kmeans( model_id="kmean", 
 training_frame=train, x=predictors,k=10, 
  max_iterations =200, standardize = TRUE, init = c(
  "Random"), seed=165,)

summary(kmean)
#assign clusters
cluster= h2o.predict(kmean, train)
train_bayes=h2o.cbind(train,cluster)
response <- "OutcomeType"
predictors <- setdiff(names(train_bayes), response)
predictors





#Bayes=h2o.naiveBayes(  model_id="Bayes", 
# training_frame=train_bayes,  ignore_const_cols = TRUE,
#  laplace = 0, threshold = 0.001, eps = 0, compute_metrics = TRUE,
#  max_runtime_secs = 0,  x=predictors,
#  y=response)
#print(Bayes)

train_master <- dat_h2o[1:26729, ]
testsubmit  <- dat_h2o[26730:nrow(dat_h2o), ]

splits = h2o.splitFrame(train_master, c(0.6,0.2), seed=1234) #split into train and test
train  = h2o.assign(splits[[1]], "train.hex") # 60%
valid  = h2o.assign(splits[[2]], "valid.hex") # 20%
test   = h2o.assign(splits[[3]], "test.hex")  # 20%
attach(train)
response <- "OutcomeType"
predictors <- setdiff(names(train), response)
predictors

#mboost <- h2o.gbm(training_frame=train,   model_id="mboost",
# validation_frame=valid,   
# x=predictors,   
#  y=response,
#seed=159
#   , ntrees = 500, max_depth = 10, min_rows = 10,learn_rate=.0001,
#  stopping_metric="misclassification",   stopping_tolerance=0.1)


#Stacking has poor preformance
#train_agg=(test_mboost*test_bayes)^.5
#train=h2o.cbind(train[["OutcomeType"]],train_agg[,-1])
#response <- "OutcomeType"
#predictors <- setdiff(names(train), response)
#predictors

#### Random Hyper-Parameter Search

hyper_params <- list(
  activation=c("Rectifier","Tanh","Maxout","RectifierWithDropout","TanhWithDropout","MaxoutWithDropout"),
  hidden=list(c(20,20),c(100,75,50),c(25,25,25,25),c(2,4,6,8,6,4,2),c(2000,1000,500)),
  input_dropout_ratio=seq(.4,.6,by=.01), #For ensemble effect see Hinton's work for explaination
  l1=seq(0,1e-4,1e-6),
  l2=seq(0,1e-4,1e-6),               
  rate=seq(0.001,.7,by=.001) ,
  rate_annealing=seq(0,2e-4,by= 1e-5)

)
hyper_params


## Stop once the top 5 models are within 1% of each other (i.e., the windowed average varies less than 1%)
search_criteria = list(strategy = "RandomDiscrete", max_runtime_secs = 360, max_models = 100, seed=1234567, stopping_rounds=10, stopping_tolerance=1e-2)
dl_random_grid <- h2o.grid(
  algorithm="deeplearning",
  grid_id = "dl_grid_random",
  training_frame=train,
  validation_frame=valid, 
  x=predictors, 
  y=response,
  epochs=20,
  loss="CrossEntropy",
  stopping_metric="logloss",
  stopping_tolerance=1e-3,        ## stop when logloss does not improve by >=1% for 2 scoring events
  stopping_rounds=4,
  score_validation_samples=500, ## downsample validation set for faster scoring
  score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
  max_w2=5,                      ## can help improve stability for Rectifier
  hyper_params = hyper_params,
  search_criteria = search_criteria,
  variable_importances=T,
  standardize=TRUE
#  ,
#  pretrained_autoencoder="ae_model"
)            



#examine grid
grid <- h2o.getGrid("dl_grid_random",sort_by="logloss",decreasing=FALSE)
grid

grid@summary_table[1,]
best_model <- h2o.getModel(grid@model_ids[[1]]) ## model with lowest logloss
plot(best_model)


#without RF variable reduction logloss ~.8,~.75 with reduction
test_mboost <- h2o.predict(mboost, testsubmit)
h2o_yhat_test_dl <- h2o.predict(best_model, testsubmit)
test_bayes <- h2o.predict(Bayes, testsubmit)

test_id <- read.csv('../input/test.csv', stringsAsFactors = F)

#test_agg=(test_mboost*test_bayes*h2o_yhat_test_dl )^(1/3)
#test_agg <- as.data.frame(test_agg) 
#test_agg =test_agg [,-1]
#test_agg =cbind(as.data.frame(test_id[,1]),test_agg )
#colnames(test_agg)[1] <- "ID"


h2o_yhat_test_dl <- as.data.frame(h2o_yhat_test_dl) 
h2o_yhat_test_dl =test_agg [,-1]
h2o_yhat_test_dl =cbind(as.data.frame(test_id[,1]),h2o_yhat_test_dl )
colnames(h2o_yhat_test_dl)[1] <- "ID"
write.csv(h2o_yhat_test_dl,file="submission.csv", row.names=FALSE)


