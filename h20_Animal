library(readr) # CSV file I/O, e.g. the read_csv function
library(h2o) #deep learning
library(caret) #preprocessing
library(dplyr) # data manipulation
library(lubridate) # dates
library(rpart) # rpart for imputation
#Load Data


# Read the data
train <- read.csv('../input/train.csv', stringsAsFactors = F)
test <- read.csv('../input/test.csv', stringsAsFactors = F)


##Preprocessing taken from Megan Risdal's "Quick and Dirty RandomForest"
# Rename the ID column so train & test match
names(train)[1] <- 'ID'

# And change ID in test to character
test$ID <- as.character(test$ID)

# Combine test & training data
full <- bind_rows(train, test)

# Get the time value:
full$TimeValue <- sapply(full$AgeuponOutcome,  
                      function(x) strsplit(x, split = ' ')[[1]][1])

# Now get the unit of time:
full$UnitofTime <- sapply(full$AgeuponOutcome,  
                      function(x) strsplit(x, split = ' ')[[1]][2])

# Fortunately any "s" marks the plural, so we can just pull them all out
full$UnitofTime <- gsub('s', '', full$UnitofTime)

full$TimeValue  <- as.numeric(full$TimeValue)
full$UnitofTime <- as.factor(full$UnitofTime)

# Make a multiplier vector
multiplier <- ifelse(full$UnitofTime == 'day', 1,
              ifelse(full$UnitofTime == 'week', 7,
              ifelse(full$UnitofTime == 'month', 30, # Close enough
              ifelse(full$UnitofTime == 'year', 365, NA))))

# Apply our multiplier
full$AgeinDays <- full$TimeValue * multiplier

# Replace blank names with "Nameless"
full$Name <- ifelse(nchar(full$Name)==0, 'Nameless', full$Name)

# Make a name v. no name variable
full$HasName[full$Name == 'Nameless'] <- 0
full$HasName[full$Name != 'Nameless'] <- 1

# Replace blank sex with most common
full$SexuponOutcome <- ifelse(nchar(full$SexuponOutcome)==0, 
                              'Spayed Female', full$SexuponOutcome)

# Extract time variables from date (uses the "lubridate" package)
full$Hour    <- hour(full$DateTime)
full$Weekday <- wday(full$DateTime)
full$Month   <- month(full$DateTime)
full$Year    <- year(full$DateTime)

# Time of day may also be useful
full$TimeofDay <- ifelse(full$Hour > 5 & full$Hour < 11, 'morning',
                  ifelse(full$Hour > 10 & full$Hour < 16, 'midday',
                  ifelse(full$Hour > 15 & full$Hour < 20, 'lateday', 'night')))

# Put factor levels into the order we want
full$TimeofDay <- factor(full$TimeofDay, 
                    levels = c('morning', 'midday',
                               'lateday', 'night'))

# Take a look as some of the levels
levels(factor(full$Breed))[1:10]

# Use "grepl" to look for "Mix"
full$IsMix <- ifelse(grepl('Mix', full$Breed), 1, 0)

# Split on "/" and remove " Mix" to simplify Breed
full$SimpleBreed <- sapply(full$Breed, 
                      function(x) gsub(' Mix', '', 
                        strsplit(x, split = '/')[[1]][1]))
                        
                        
                        
# Use strsplit to grab the first color
full$SimpleColor <- sapply(full$Color, 
                      function(x) strsplit(x, split = '/| ')[[1]][1])
levels(factor(full$SimpleColor))
# Use "grepl" to look for "Intact"
full$Intact <- ifelse(grepl('Intact', full$SexuponOutcome), 1,
               ifelse(grepl('Unknown', full$SexuponOutcome), 'Unknown', 0))

# Use "grepl" to look for sex
full$Sex <- ifelse(grepl('Male', full$SexuponOutcome), 'Male',
            ifelse(grepl('Unknown', full$Sex), 'Unknown', 'Female'))
            
# Use rpart to predict the missing age values
age_fit <- rpart(AgeinDays ~ AnimalType + Sex + Intact + SimpleBreed + HasName, 
              data = full[!is.na(full$AgeinDays), ], 
              method = 'anova')

# Impute predicted age values where missing using "predict"
full$AgeinDays[is.na(full$AgeinDays)] <- predict(age_fit, full[is.na(full$AgeinDays), ])

# All gone? Yes.
sum(is.na(full$AgeinDays))

# Use the age variable to make a puppy/kitten variable
full$Lifestage[full$AgeinDays < 365] <- 'baby'
full$Lifestage[full$AgeinDays >= 365] <- 'adult'

full$Lifestage <- factor(full$Lifestage)

factorVars <- c('Name','OutcomeType','OutcomeSubtype','AnimalType',
                'SexuponOutcome','AgeuponOutcome','SimpleBreed','SimpleColor',
                'HasName','IsMix','Intact','Sex','TimeofDay','Lifestage')

full[factorVars] <- lapply(full[factorVars], function(x) as.factor(x))





#Start up h20 cluster for deep learning


h2o.init(nthreads = -1,min_mem_size="1G")
dat_h2o = as.h2o(full) #Convert to h2o dataframe




dat_h2o=dat_h2o[,-5] #remove to prevent leakage
dat_h2o=dat_h2o[,-3]#data is elsewhere
dat_h2o=dat_h2o[,-2]#no reason to suppose this is a predictor
dat_h2o=dat_h2o[,-1]#no reason to suppose this is a predictor

# Split up train and test data
train_master <- dat_h2o[1:26729, ]
testsubmit  <- dat_h2o[26730:nrow(dat_h2o), ]

splits = h2o.splitFrame(train_master, c(0.6,0.2), seed=1234) #split into train and test
train  = h2o.assign(splits[[1]], "train.hex") # 60%
valid  = h2o.assign(splits[[2]], "valid.hex") # 20%
test   = h2o.assign(splits[[3]], "test.hex")  # 20%

response <- "OutcomeType"
predictors <- setdiff(names(train), response)
predictors


#Try first model
#m1 <- h2o.deeplearning(
#  model_id="dl_model_first", 
#  training_frame=train, 
#  validation_frame=valid,   ## validation dataset: used for scoring and early stopping
#  x=predictors,
#  y=response,
#  activation="Rectifier",  ## default
#  hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each
#  epochs=1,
#  variable_importances=T    ## not enabled by default
#)


#examine model
#plot(m1)
#summary(m1)

#Logloss ~1, validation and train sets close

#Autoencoder
#ae_model = h2o.deeplearning(model_id="ae",x = 1:ncol(train), standardize=TRUE, training_frame = train, autoencoder = TRUE,
#hidden = c(10, 10), epochs = 5,activation="Tanh",variable_importances=T)
#Note that highly importance variables in the autoencoded model signify clusters.
#h2o.varimp(ae_model)


#Try gbm


hyper_parameters <- list(
ntrees=c(50,100,150),
max_depth=c(5,10,15),
learn_rate=c(.1,.25,.5,.75,1)
)


grid <- h2o.grid("gbm",  grid_id = "gbm_grid",
hyper_params = hyper_parameters, 
y = response, x = predictors, distribution="multinomial", seed=159,
training_frame =train, validation_frame = valid, stopping_tolerance=0.1,
stopping_metric="logloss")

grid <- h2o.getGrid("gbm_grid",sort_by="logloss",decreasing=FALSE)
grid

grid@summary_table[1,]
best_model <- h2o.getModel(grid@model_ids[[1]]) ## model with lowest logloss
best_model_gbm
plot(best_model_gbm)


#### Random Hyper-Parameter Search

hyper_params <- list(
  activation=c("Rectifier","Tanh","Maxout","RectifierWithDropout","TanhWithDropout","MaxoutWithDropout"),
  hidden=list(c(20,20),c(50,50),c(30,30,30),c(25,25,25,25)),
  input_dropout_ratio=.5, #For ensemble effect see Hinton's work for explaination
  dropout_ratio=.5,
  rate=seq(.3,.95,by=.05),
  l1=seq(0,1e-4,1e-6),
  l2=seq(0,1e-4,1e-6)
)
hyper_params


## Stop once the top 5 models are within 1% of each other (i.e., the windowed average varies less than 1%)
search_criteria = list(strategy = "RandomDiscrete", max_runtime_secs = 360, max_models = 100, seed=1234567, stopping_rounds=5, stopping_tolerance=1e-2)
dl_random_grid <- h2o.grid(
  algorithm="deeplearning",
  grid_id = "dl_grid_random",
  training_frame=train,
  validation_frame=valid, 
  x=predictors, 
  y=response,
  epochs=10,
  loss="CrossEntropy",
  stopping_metric="logloss",
  stopping_tolerance=1e-2,        ## stop when logloss does not improve by >=1% for 2 scoring events
  stopping_rounds=2,
  score_validation_samples=120, ## downsample validation set for faster scoring
  score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
  max_w2=10,                      ## can help improve stability for Rectifier
  hyper_params = hyper_params,
  search_criteria = search_criteria,
  variable_importances=T,
  standardize=TRUE
)            



#examine grid
grid <- h2o.getGrid("dl_grid_random",sort_by="logloss",decreasing=FALSE)
grid

grid@summary_table[1,]
best_model <- h2o.getModel(grid@model_ids[[1]]) ## model with lowest logloss
best_model
plot(best_model)




h2o_yhat_test <- h2o.predict(best_model, testsubmit)
nrow(as.data.frame(h2o_yhat_test)) #sanity check
test_id <- read.csv('../input/test.csv', stringsAsFactors = F)

df_yhat_test <- as.data.frame(h2o_yhat_test) 
df_yhat_test =df_yhat_test [,-1]
df_yhat_test =cbind(as.data.frame(test_id[,1]),df_yhat_test )
write.csv(df_yhat_test,file="submission.csv")


