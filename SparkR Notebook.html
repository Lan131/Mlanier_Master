<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>SparkR Notebook - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/latest.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","maxCustomTags":45,"enableInstanceProfilesUIInJobs":false,"nodeInfo":{"node_types":[{"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"memory_mb":6144,"category":"Community Edition","num_cores":0.88,"support_ebs_volumes":false}],"default_node_type_id":"dev-tier-node"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":false,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"clusters":true,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-603ee96315e2763ac658f02088fa637d69d69ac26d76ce791f1f65cd91bd53cd","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-10257b76b554bf22b7de2a0f67afe519b744d857b68ac8d74b9ef1b6e1fbedc6","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-8c3d33f199454d80c281382c3570e573f07b23efff3a3c3e83397bb1863218ed","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-efd93e665ef65c872361afb705f21d03af37880940ddadf59dac3245ec83063d","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-f40841fbebd0e733dfd15b2913bc495542eacb05ba2efa43cd192aed8b307046","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-a312e6077d104638f7c65e5fc6dd82bc2ffc329c7afddcd406dcf249ea388a13","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":false,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":8,"memory-optimized":1,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":false,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.32","accountsLimit":3,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"disableLegacyDashboards":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"5e0e1a3d13cb2f419637c68b4eb9acf54fc74090","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"enableTerminal":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3803623629678289,"name":"SparkR Notebook","language":"r","commands":[{"version":"CommandV1","origId":3803623629678291,"guid":"f87f5873-4f96-40ac-bacd-416f431ff230","subtype":"command","commandType":"auto","position":1.0,"command":"library(dplyr)\ninstall.packages('magrittr')\nlibrary(magrittr)\n                \n\n\n","commandVersion":0,"state":"error","results":null,"errorSummary":"<code style=\"font-size:10p\"> Installing package into ‘/databricks/spark/R/lib’ </code>","error":"<pre style=\"font-size:10p\">Installing package into ‘/databricks/spark/R/lib’\n(as ‘lib’ is unspecified)\ntrying URL 'http://cran.us.r-project.org/src/contrib/magrittr_1.5.tar.gz'\nContent type 'application/x-gzip' length 200504 bytes (195 KB)\n==================================================\ndownloaded 195 KB\n\n* installing *source* package ‘magrittr’ ...\n** package ‘magrittr’ successfully unpacked and MD5 sums checked\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded\n* DONE (magrittr)\nError in f(...) : unexpected type: SparkDataFrame\nIn addition: Warning messages:\n1: 'read.df(sqlContext...)' is deprecated.\nUse 'read.df(path = NULL, source = NULL, schema = NULL, ...)' instead.\nSee help(&quot;Deprecated&quot;) \n2: 'createDataFrame(sqlContext...)' is deprecated.\nUse 'createDataFrame(data, schema = NULL)' instead.\nSee help(&quot;Deprecated&quot;) </pre>","workflows":[],"startTime":1.479700647432E12,"submitTime":1.479700647432E12,"finishTime":1.479700652289E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f0afdfd8-555c-4584-8f50-e8c8199d332f"},{"version":"CommandV1","origId":889440608602931,"guid":"c6729429-4166-40fb-b09a-8f8d4dcbf9c0","subtype":"command","commandType":"auto","position":1.25,"command":"sparkDF <- read.df(sqlContext, \"/FileStore/tables/gqmstiwl1479676671412/With_interaction.csv\", source = \"csv\", header=\"true\", inferSchema = \"true\")\nclass(sparkDF)\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>[1] \"SparkDataFrame\"\nattr(,\"package\")\n[1] \"SparkR\"</pre>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<code style=\"font-size:10p\"> Error in eval(expr, envir, enclos) : could not find function &quot;type&quot; </code>","error":"<pre style=\"font-size:10p\">Error in eval(expr, envir, enclos) : could not find function &quot;type&quot;\nIn addition: Warning message:\n'read.df(sqlContext...)' is deprecated.\nUse 'read.df(path = NULL, source = NULL, schema = NULL, ...)' instead.\nSee help(&quot;Deprecated&quot;) </pre>","workflows":[],"startTime":1.479704108972E12,"submitTime":1.479704101833E12,"finishTime":1.479704110905E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"163aeaf8-042d-4d55-bcff-465498929fde"},{"version":"CommandV1","origId":2844983327515221,"guid":"b92d8079-e626-424c-bf31-187968c22a96","subtype":"command","commandType":"auto","position":1.75,"command":"\nhead(where(sparkDF, sparkDF$loss > .5))\n\n\n ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 cat10 cat11 cat12 cat13 cat14\n1  1    A    B    A    B    A    A    A    A    B     A     B     A     A     A\n2  2    A    B    A    A    A    A    A    A    B     B     A     A     A     A\n3  5    A    B    A    A    B    A    A    A    B     B     B     B     B     A\n4 10    B    B    A    B    A    A    A    A    B     A     A     A     A     A\n5 11    A    B    A    B    A    A    A    A    B     B     A     B     A     A\n6 13    A    B    A    A    A    A    A    A    B     A     A     A     A     A\n  cat15 cat16 cat17 cat18 cat19 cat20 cat21 cat22 cat23 cat24 cat25 cat26 cat27\n1     A     A     A     A     A     A     A     A     B     A     A     A     A\n2     A     A     A     A     A     A     A     A     A     A     A     A     A\n3     A     A     A     A     A     A     A     A     A     A     A     A     A\n4     A     A     A     A     A     A     A     A     B     A     A     A     A\n5     A     A     A     A     A     A     A     A     B     A     A     A     A\n6     A     A     A     A     A     A     A     A     A     A     A     A     A\n  cat28 cat29 cat30 cat31 cat32 cat33 cat34 cat35 cat36 cat37 cat38 cat39 cat40\n1     A     A     A     A     A     A     A     A     A     A     A     A     A\n2     A     A     A     A     A     A     A     A     A     A     A     A     A\n3     A     A     A     A     A     A     A     A     B     A     A     A     A\n4     A     A     A     A     A     A     A     A     A     A     A     A     A\n5     A     A     A     A     A     A     A     A     A     A     A     A     A\n6     A     A     A     A     A     A     A     A     A     A     A     A     A\n  cat41 cat42 cat43 cat44 cat45 cat46 cat47 cat48 cat49 cat50 cat51 cat52 cat53\n1     A     A     A     A     A     A     A     A     A     A     A     A     A\n2     A     A     A     A     A     A     A     A     A     A     A     A     A\n3     A     A     A     A     A     A     A     A     A     A     A     A     A\n4     A     A     A     A     A     A     A     A     A     A     A     A     A\n5     A     A     A     A     A     A     A     A     A     A     A     A     A\n6     A     A     A     A     A     A     A     A     A     A     A     A     A\n  cat54 cat55 cat56 cat57 cat58 cat59 cat60 cat61 cat62 cat63 cat64 cat65 cat66\n1     A     A     A     A     A     A     A     A     A     A     A     A     A\n2     A     A     A     A     A     A     A     A     A     A     A     A     A\n3     A     A     A     A     A     A     A     A     A     A     A     A     A\n4     A     A     A     A     A     A     A     A     A     A     A     A     A\n5     A     A     A     A     A     A     A     A     A     A     A     A     A\n6     A     A     A     A     A     A     A     A     A     A     A     A     A\n  cat67 cat68 cat69 cat70 cat71 cat72 cat73 cat74 cat75 cat76 cat77 cat78 cat79\n1     A     A     A     A     A     A     A     A     B     A     D     B     B\n2     A     A     A     A     A     A     A     A     A     A     D     B     B\n3     A     A     A     A     A     A     A     A     A     A     D     B     B\n4     A     A     A     A     A     A     B     A     A     A     D     B     B\n5     A     A     A     A     A     B     A     A     A     A     D     B     D\n6     A     A     A     A     A     B     A     A     A     A     D     B     D\n  cat80 cat81 cat82 cat83 cat84 cat85 cat86 cat87 cat88 cat89 cat90 cat91 cat92\n1     D     D     B     D     C     B     D     B     A     A     A     A     A\n2     D     D     A     B     C     B     D     B     A     A     A     A     A\n3     B     D     B     D     C     B     B     B     A     A     A     A     A\n4     D     D     D     B     C     B     D     B     A     A     A     A     A\n5     B     D     B     B     C     B     B     C     A     A     A     B     H\n6     B     D     B     B     C     B     B     B     A     A     A     A     A\n  cat93 cat94 cat95 cat96 cat97 cat98 cat99 cat100 cat101 cat102 cat103 cat104\n1     D     B     C     E     A     C     T      B      G      A      A      I\n2     D     D     C     E     E     D     T      L      F      A      A      E\n3     D     D     C     E     E     A     D      L      O      A      B      E\n4     D     D     C     E     E     D     T      I      D      A      A      E\n5     D     B     D     E     E     A     P      F      J      A      A      D\n6     D     D     D     E     C     A     P      J      D      A      A      E\n  cat105 cat106 cat107 cat108 cat109 cat110 cat111 cat112 cat113 cat114 cat115\n1      E      G      J      G     BU     BC      C     AS      S      A      O\n2      E      I      K      K     BI     CQ      A     AV     BM      A      O\n3      F      H      F      A     AB     DK      A      C     AF      A      I\n4      E      I      K      K     BI     CS      C      N     AE      A      O\n5      E      K      G      B      H      C      C      Y     BM      A      K\n6      E      H      F      B     BI     CS      A     AS     AE      A      K\n  cat116    cont1    cont2    cont3    cont4    cont5    cont6    cont7   cont8\n1     LB 0.726300 0.245921 0.187583 0.789639 0.310061 0.718367 0.335060 0.30260\n2     DP 0.330514 0.737068 0.592681 0.614134 0.885834 0.438917 0.436585 0.60087\n3     GK 0.261841 0.358319 0.484196 0.236924 0.397069 0.289648 0.315545 0.27320\n4     DJ 0.321594 0.555782 0.527991 0.373816 0.422268 0.440945 0.391128 0.31796\n5     CK 0.273204 0.159990 0.527991 0.473202 0.704268 0.178193 0.247408 0.24564\n6     DJ 0.546670 0.681761 0.634224 0.373816 0.302678 0.364464 0.401162 0.26847\n    cont9  cont10   cont11   cont12   cont13   cont14  int(7,12)    loss\n1 0.67135 0.83510 0.569745 0.594646 0.822493 0.714843 0.19924209 2213.18\n2 0.35127 0.43919 0.338312 0.366307 0.611431 0.304496 0.15992414 1283.60\n3 0.26076 0.32446 0.381398 0.373424 0.195709 0.774425 0.11783208 3005.09\n4 0.32128 0.44467 0.327915 0.321570 0.605077 0.602642 0.12577503  939.85\n5 0.22089 0.21230 0.204687 0.202213 0.246011 0.432606 0.05002911 2763.85\n6 0.46226 0.50556 0.366788 0.359249 0.345247 0.726792 0.14411705 5142.87</pre>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<code style=\"font-size:10p\"> Error in head(select(sparkDF_sp, sparkDF_sp$cat111)) :  </code>","error":"<pre style=\"font-size:10p\">Error in head(select(sparkDF_sp, sparkDF_sp$cat111)) : \n  error in evaluating the argument 'x' in selecting a method for function 'head': Error in select_(.data, .dots = lazyeval::lazy_dots(...)) : \n  object 'sparkDF_sp' not found\nCalls: select -&gt; select_\n</pre>","workflows":[],"startTime":1.479703669358E12,"submitTime":1.479703662227E12,"finishTime":1.479703672544E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0259b89d-66ff-4026-858f-ac60fb117c34"},{"version":"CommandV1","origId":889440608602934,"guid":"53762b93-f98c-451c-a78d-6f63481e2dd0","subtype":"command","commandType":"auto","position":1.8125,"command":"createOrReplaceTempView(sparkDF, \"sparkDF\")\navg_loss <- sql(\"SELECT avg(loss) FROM sparkDF\")\nhead(avg_loss)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  avg(loss)\n1  3037.338</pre>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.47970367606E12,"submitTime":1.479703668931E12,"finishTime":1.479703679272E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1ee795f1-8726-40f2-8945-3cf27d09f426"},{"version":"CommandV1","origId":889440608602935,"guid":"1b09b08b-cba7-4381-9e04-1c26ad6b4326","subtype":"command","commandType":"auto","position":1.84375,"command":"head(summary( sparkDF))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  summary                 id               cont1               cont2\n1   count             188318              188318              188318\n2    mean  294135.9825614121  0.4938613645642005  0.5071883561794936\n3  stddev 169336.08486658792 0.18764017641388603 0.20720173860981386\n4     min                  1              1.6E-5            0.001149\n5     max             587633            0.984975            0.862654\n                cont3               cont4               cont5\n1              188318              188318              188318\n2 0.49891845072165986  0.4918123025892125 0.48742772878327495\n3 0.20210460819343742 0.21129221269283577 0.20902682854450394\n4            0.002634            0.176921            0.281143\n5            0.944251            0.954297            0.983674\n                cont6               cont7               cont8\n1              188318              188318              188318\n2 0.49094453373550273  0.4849702050680173 0.48643731586994415\n3 0.20527256983553038 0.17845016396070926 0.19937045456133265\n4            0.012683            0.069503             0.23688\n5            0.997162                 1.0              0.9802\n                cont9              cont10              cont11\n1              188318              188318              188318\n2  0.4855063198950671 0.49806585042322404  0.4935110085546688\n3 0.18166017135075618  0.1858767259320183 0.20973651144747796\n4              8.0E-5                 0.0            0.035321\n5              0.9954             0.99498            0.998742\n               cont12              cont13              cont14\n1              188318              188318              188318\n2 0.49315042562582034 0.49313761583599725 0.49571701797491136\n3 0.20942662107602905 0.21277724232240955 0.22248753955922554\n4            0.036232             2.28E-4            0.179722\n5            0.998484            0.988494            0.844848\n            int(7,12)               loss\n1              188318             188318\n2 0.26691991794301145 3037.3376856699924\n3 0.19080712285708876  2904.086186390403\n4         0.002518233               0.67\n5            0.998484          121012.25</pre>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<code style=\"font-size:10p\"> Error in UseMethod(&quot;summarise_&quot;) :  </code>","error":"<pre style=\"font-size:10p\">Error in UseMethod(&quot;summarise_&quot;) : \n  no applicable method for 'summarise_' applied to an object of class &quot;SparkDataFrame&quot;</pre>","workflows":[],"startTime":1.47970368696E12,"submitTime":1.479703679832E12,"finishTime":1.479703694088E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3e84d2c3-d6d8-46ee-864c-e0f062d52376"},{"version":"CommandV1","origId":889440608602932,"guid":"18971964-03e7-44f5-99fd-b4589e57c841","subtype":"command","commandType":"auto","position":1.875,"command":"head(summarize(groupBy(sparkDF, sparkDF$cat111), count = n(sparkDF$cat111)))\n\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  cat111 count\n1      K  1353\n2      F     3\n3      Q    91\n4      E 14682\n5      B     7\n6      Y     2</pre>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<code style=\"font-size:10p\"> Error in head(summarize(groupBy(sparkDF, sparkDF$cat111), count = n(sparkDF$cat111))) :  </code>","error":"<pre style=\"font-size:10p\">Error in head(summarize(groupBy(sparkDF, sparkDF$cat111), count = n(sparkDF$cat111))) : \n  error in evaluating the argument 'x' in selecting a method for function 'head': Error in UseMethod(&quot;summarise_&quot;) : \n  no applicable method for 'summarise_' applied to an object of class &quot;GroupedData&quot;\nCalls: summarize -&gt; summarise_\n</pre>","workflows":[],"startTime":1.479703733758E12,"submitTime":1.479703726625E12,"finishTime":1.479703735962E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"266a9c17-40ab-4a27-87c4-0760d0965eb4"},{"version":"CommandV1","origId":889440608602937,"guid":"d97aa344-c5ef-46a3-959e-2ca01dccadac","subtype":"command","commandType":"auto","position":4.0,"command":"GLM <- spark.glm(sparkDF, loss ~ ., family = \"gaussian\")\nsummary(GLM)\n","commandVersion":0,"state":"error","results":null,"errorSummary":"Internal error, sorry. Attach your notebook to a different cluster or restart the current cluster.","error":"java.lang.Exception: Could not stop computations\n\tat com.databricks.backend.daemon.driver.DriverClient$$anonfun$com$databricks$backend$daemon$driver$DriverClient$$poll$2.apply(DriverClient.scala:263)\n\tat com.databricks.backend.daemon.driver.DriverClient$$anonfun$com$databricks$backend$daemon$driver$DriverClient$$poll$2.apply(DriverClient.scala:253)\n\tat scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)\n\tat scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n\tat com.databricks.threading.NamedExecutor$$anonfun$com$databricks$threading$NamedExecutor$$execute0$1$$anonfun$apply$mcV$sp$1$$anon$1$$anonfun$run$1.apply$mcV$sp(NamedExecutor.scala:238)\n\tat com.databricks.threading.NamedExecutor$$anonfun$com$databricks$threading$NamedExecutor$$execute0$1$$anonfun$apply$mcV$sp$1$$anon$1$$anonfun$run$1.apply(NamedExecutor.scala:238)\n\tat com.databricks.threading.NamedExecutor$$anonfun$com$databricks$threading$NamedExecutor$$execute0$1$$anonfun$apply$mcV$sp$1$$anon$1$$anonfun$run$1.apply(NamedExecutor.scala:238)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:145)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:140)\n\tat com.databricks.threading.NamedExecutor.withAttributionContext(NamedExecutor.scala:190)\n\tat com.databricks.threading.NamedExecutor$$anonfun$com$databricks$threading$NamedExecutor$$execute0$1$$anonfun$apply$mcV$sp$1$$anon$1.run(NamedExecutor.scala:237)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n","workflows":[],"startTime":1.479703735774E12,"submitTime":1.479703735774E12,"finishTime":1.479703964705E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ef60782f-9c26-46a6-a56b-3d3df3930e2a"},{"version":"CommandV1","origId":889440608602938,"guid":"61b78336-a1ab-4d5f-a263-179b4bc7d0dd","subtype":"command","commandType":"auto","position":5.0,"command":"createOrReplaceTempView(sparkDF, \"sparkDF\")\navg_loss <- sql(\"SELECT avg(loss),cat111 FROM sparkDF group by cat111\")\nhead(avg_loss)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>  avg(loss) cat111\n1  5175.608      K\n2  3365.923      F\n3  6552.889      Q\n4  3487.309      E\n5  2578.653      B\n6  5891.655      Y</pre>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.479703717761E12,"submitTime":1.479703710624E12,"finishTime":1.479703720758E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8e8df3e1-127d-4345-9f4f-9286ee6222c4"},{"version":"CommandV1","origId":889440608602939,"guid":"7ad628a7-3a2f-43aa-8063-f8c2cf2484fa","subtype":"command","commandType":"auto","position":6.0,"command":"GLM <- spark.glm(sparkDF, loss ~ cat111 + cont1, family = \"gaussian\")\nsummary(GLM)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<pre style=\"font-size:10p\"></pre><pre style = 'font-size:10pt'>\nDeviance Residuals: \n(Note: These are approximate quantiles with relative error <= 0.01)\n   Min      1Q  Median      3Q     Max  \n -8897   -1743    -920     889  116970  \n\nCoefficients:\n             Estimate  Std. Error  t value   Pr(>|t|) \n(Intercept)  6030.2    2027.9      2.9736    0.0029436\ncat111_A     -3123.7   2027.7      -1.5405   0.12344  \ncat111_C     -2837.7   2027.8      -1.3994   0.16169  \ncat111_E     -2463.7   2027.9      -1.2149   0.22439  \ncat111_G     -1936.7   2028        -0.95496  0.3396   \ncat111_I     -1412.7   2028.3      -0.69649  0.48612  \ncat111_K     -773.98   2029.2      -0.38142  0.70289  \ncat111_M     140.36    2032        0.069076  0.94493  \ncat111_O     145.25    2036.9      0.07131   0.94315  \ncat111_Q     601.71    2049.9      0.29354   0.76911  \ncat111_S     1885.9    2080.4      0.90652   0.36466  \ncat111_U     3658.6    2150.7      1.7011    0.088918 \ncat111_W     3540.2    2150.7      1.646     0.099758 \ncat111_B     -3393.6   2299.2      -1.476    0.13996  \ncat111_F     -2581.2   2617.7      -0.98603  0.32412  \ncat111_D     -1295.2   2617.7      -0.49478  0.62075  \ncont1        -161.12   35.22       -4.5748   4.769e-06\n\n(Dispersion parameter for gaussian family taken to be 8222909)\n\n    Null deviance: 1.5882e+12  on 188317  degrees of freedom\nResidual deviance: 1.5484e+12  on 188301  degrees of freedom\nAIC: 3532923\n\nNumber of Fisher Scoring iterations: 1\n</pre>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<code style=\"font-size:10p\"> Error in handleErrors(returnStatus, conn) :  </code>","error":"<pre style=\"font-size:10p\">Error in handleErrors(returnStatus, conn) : \n  java.lang.IllegalArgumentException: Could not parse formula: sparkDF$loss ~ sparkDF$cat111 + sparkDF$cont1\n\tat org.apache.spark.ml.feature.RFormulaParser$.parse(RFormulaParser.scala:200)\n\tat org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:117)\n\tat org.apache.spark.ml.r.GeneralizedLinearRegressionWrapper$.fit(GeneralizedLinearRegressionWrapper.scala:74)\n\tat org.apache.spark.ml.r.GeneralizedLinearRegressionWrapper.fit(GeneralizedLinearRegressionWrapper.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:172)\n\tat org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:109)\n\tat org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:41)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.ReadTimeoutHandler.channelRead(ReadTimeoutHandler.java:152)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)\n\tat java.lang.Thread.run(Thread.java:745)\n</pre>","workflows":[],"startTime":1.479704153177E12,"submitTime":1.479704146038E12,"finishTime":1.479704168905E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8c62c41a-3911-4687-b0ac-5ef6ecf4c295"}],"dashboards":[],"guid":"f138efd8-10f5-45f5-872c-0531589fb366","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
